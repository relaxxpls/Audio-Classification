{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Audio Classification\r\n",
    "\r\n",
    "## Basic steps\r\n",
    "\r\n",
    "* Load model\r\n",
    "* Classify test data\r\n",
    "\r\n",
    "## Usage examples at the end\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import sklearn.metrics as metrics\r\n",
    "import torch\r\n",
    "import torch.nn as nn\r\n",
    "import torch.nn.functional as F\r\n",
    "import torchaudio\r\n",
    "import torchaudio.transforms as aT\r\n",
    "import torchvision.models as models\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "class DenseNet(nn.Module):\r\n",
    "    def __init__(self, num_classes):\r\n",
    "        super().__init__()\r\n",
    "\r\n",
    "        self.model = models.densenet201(pretrained=True)\r\n",
    "        conv0 = self.model.features.conv0\r\n",
    "        self.model.features.conv0 = nn.Conv2d(\r\n",
    "            1,\r\n",
    "            conv0.out_channels,\r\n",
    "            kernel_size=conv0.kernel_size,\r\n",
    "            stride=conv0.stride,\r\n",
    "            padding=conv0.padding,\r\n",
    "        )\r\n",
    "        self.model.classifier = nn.Linear(1920, num_classes)\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        output = self.model(x)\r\n",
    "        return output\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\r\n",
    "print(device)\r\n",
    "\r\n",
    "classes = ['1', '2', '3', '4', '5', '6', '7']\r\n",
    "\r\n",
    "\r\n",
    "def audio_loader(path, max_length_in_seconds=4):\r\n",
    "    waveform, sample_rate = torchaudio.load(path)\r\n",
    "    _, num_frames = waveform.shape\r\n",
    "    max_frames = sample_rate * max_length_in_seconds\r\n",
    "\r\n",
    "    # ? Pad audio with zeros if too short or cut audio if too long\r\n",
    "    if num_frames < max_frames:\r\n",
    "        waveform = torch.nn.functional.pad(waveform, (0, max_frames - num_frames))\r\n",
    "    elif num_frames > max_frames:\r\n",
    "        waveform = waveform.narrow(dim=1, start=0, length=max_frames)\r\n",
    "\r\n",
    "    transforms = nn.Sequential(\r\n",
    "        aT.Resample(44100, 22050),\r\n",
    "        aT.MFCC(sample_rate=sample_rate, n_mfcc=64),\r\n",
    "        aT.AmplitudeToDB(),\r\n",
    "    )\r\n",
    "    waveform = transforms(waveform)\r\n",
    "\r\n",
    "    return waveform\r\n",
    "\r\n",
    "\r\n",
    "def predict(model, input_tensor):\r\n",
    "    model.eval()\r\n",
    "    inputs = input_tensor.unsqueeze(1)\r\n",
    "    inputs = inputs.to(device)\r\n",
    "\r\n",
    "    with torch.no_grad():\r\n",
    "        output = model(inputs)\r\n",
    "\r\n",
    "        output = output.squeeze()\r\n",
    "        output =  F.softmax(output, dim=-1)\r\n",
    "\r\n",
    "        accuracy, predicted = torch.max(output.data, -1)\r\n",
    "        accuracy *= 100\r\n",
    "\r\n",
    "        # ? provide class labels\r\n",
    "        predicted = classes[predicted]\r\n",
    "\r\n",
    "        return predicted, accuracy\r\n",
    "\r\n",
    "\r\n",
    "def make_predictions(audio_paths, labels):\r\n",
    "    MODEL_PATH = './IE643_190020066_CHALLENGE_MODEL.pt'\r\n",
    "    model = torch.load(MODEL_PATH, device)\r\n",
    "\r\n",
    "    predictions = []\r\n",
    "\r\n",
    "    for audio_path, label in zip(audio_paths, labels):\r\n",
    "        input_tensor = audio_loader(audio_path)\r\n",
    "        predicted, accuracy = predict(model, input_tensor)\r\n",
    "\r\n",
    "        # print(f\"label: {label}, predicted: {predicted} ({accuracy:.2f} %)\")\r\n",
    "        predictions.append(predicted)\r\n",
    "\r\n",
    "    accuracy = metrics.accuracy_score(labels, predictions) * 100\r\n",
    "\r\n",
    "    precision = metrics.precision_score(labels, predictions, average='weighted', zero_division=0)\r\n",
    "    recall = metrics.recall_score(labels, predictions, average='weighted', zero_division=0)\r\n",
    "    f1 = metrics.f1_score(labels, predictions, average='weighted', zero_division=0)\r\n",
    "\r\n",
    "    return accuracy, precision, recall, f1\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Example 1"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "audio_paths = ['./dataset/2/430816.wav', './dataset/6/112564.wav']\r\n",
    "labels = ['2', '6']\r\n",
    "\r\n",
    "accuracy, precision, recall, f1 = make_predictions(audio_paths, labels)\r\n",
    "print(f\"Accuracy: {accuracy:.3f}, Precision: {precision:.3f}, Recall: {recall:.3f}, F1: {f1:.3f}\")\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Python39\\lib\\site-packages\\torchaudio\\functional\\functional.py:432: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy: 0.000, Precision: 0.000, Recall: 0.000, F1: 0.000\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Example 2"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "from pathlib import Path\r\n",
    "\r\n",
    "audio_paths = list(Path('./dataset').glob('**/*.wav'))\r\n",
    "labels = [path.parts[-2] for path in audio_paths]\r\n",
    "\r\n",
    "accuracy, precision, recall, f1 = make_predictions(audio_paths, labels)\r\n",
    "print(f\"Accuracy: {accuracy:.3f}, Precision: {precision:.3f}, Recall: {recall:.3f}, F1: {f1:.3f}\")\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Python39\\lib\\site-packages\\torchaudio\\functional\\functional.py:432: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy: 51.261, Precision: 0.542, Recall: 0.513, F1: 0.479\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}